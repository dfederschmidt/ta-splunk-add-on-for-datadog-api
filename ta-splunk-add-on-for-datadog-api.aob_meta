{"basic_builder": {"appname": "ta-splunk-add-on-for-datadog-api", "friendly_name": "Datadog Add-on for Splunk", "version": "1.0.8", "author": "FDSE", "description": "Datadog TA used for Datadog API calls", "theme": "#a65c7d", "large_icon": "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", "small_icon": "iVBORw0KGgoAAAANSUhEUgAAACQAAAAkCAYAAADhAJiYAAADHUlEQVRYR+2YeU8qQRDEG0XEE0HxRvD2+38a8MI7XoiCCohG8+u83owIC7u++MiL/Ze6OzPVVdU1GyPSZxXpMzzS34Dy+fz7v2BsZ2fHI+YTQ7+A/sjxfzMUj8e1z0gkIq+vr9JsNgPbMDRDAwMDsrGxIfV6XWq1miSTSXl8fJSLiwsFsbi4KOPj43J3dycAHRkZkf39fXl/95+V0IASiYQ8PDz0zACsAbBarfquCQ0Idg4ODryO5+fnBZAwwOFvb29yfX0tlUrFk3FtbU3X+FUgQMg0MTEh6XRa6aey2aw8PT3J7e3tl3MAtrq6qsCQkwJUuVyW+/v7tvIFArS1tSW7u7vKAEzY7xy0ubmpRi4Wi56H8M3h4aGMjY2pXJeXl95a3t/b2/vSRCBAKysrcnJyopsgmbE0NDSkAHkOAGp7e1sKhYJMTk6qbACiEfNQLpeTo6Oj7wFCqpubG+2YkX55edENewHEe8hnDKZSKZ3A1grEkElk3bPZ1NSUxGIx3RyGzMTT09PK0NLSkjw/P6tv7H384+7hguoZEFlCV+SMMTUzMyOlUqlrtnCgyW3eyWQyyjY5FhoQi/HB8PCwbgQ7dNtLEZQ045oZ6ZnQUICYGBKZ0cczjUZDwfF3JPEr9z13GDC6xYGt71kyQCAVXbopbZ37AXKZNO8gGTHQet/1DIgDzdSEm423m0WdQLkjjsnPz88/ZVgoyViEiUlklxWS+vj42FcyAw1T3H9kFpexTV5oQMvLy3J2dqbrrevR0VG9t/BUu8JzACEW3BxyQzY0ICbE0pnsIYUZXfyAseme64OKRqOazrxDuhtLAISh9fV1b6/QgOwTYnZ2Vm9tsmlubs6TDGCnp6e6vxkfYHiOO5CCJbLLArSV1UCmdhdzCNcA3QJkcHBQs4nJofgqWFhYkKurK2UOZvCbXR2dTBcaUDtTmukNkPsxBruA7MRM4Bxq7YgDCDlyhEPIKH62OEAasguPAQTPmf/8RjI0Q+02xVNIB1im7kc/8n2D5xsP/ypD38DhLf0F1I3Fjgx1W/gTz/v7/0M/wUC3Mz4Az5AlQ2jnGgQAAAAASUVORK5CYII=", "visible": true, "tab_version": "3.0.1", "tab_build_no": "2", "build_no": 4}, "data_input_builder": {"datainputs": [{"index": "default", "sourcetype": "datadog:event:stream", "interval": "60", "use_external_validation": true, "streaming_mode_xml": true, "name": "datadog_event_stream", "title": "datadog event stream", "description": "Input for Datadog Events endpoints", "type": "customized", "parameters": [{"name": "start_time", "label": "Start Time", "help_string": "This is the start time from where you want to ingest the data. Please enter UTC time. Example Format: 2020-02-08 00:00:00", "required": true, "format_type": "text", "default_value": "", "placeholder": "2020-02-08 00:00:00", "type": "text", "value": "2020-05-10 08:00:00"}, {"name": "end_time", "label": "End Time", "help_string": "This is the end time to where you want to ingest the data. It could be a future time. Please enter UTC time. Example Format: 2030-03-08 22:11:59", "required": true, "format_type": "text", "default_value": "", "placeholder": "2030-03-08 22:11:59", "type": "text", "value": "2020-05-21 8:00:00"}, {"name": "priority", "label": "Priority", "help_string": "Priority of your events. (optional)", "required": false, "possible_values": [{"value": "low", "label": "low"}, {"value": "normal", "label": "normal"}, {"value": "none", "label": "none"}], "format_type": "radiogroup", "default_value": "none", "type": "radiogroup", "value": "none"}, {"name": "sources", "label": "Sources", "help_string": "A comma separated string of sources. (optional)", "required": false, "format_type": "text", "default_value": "", "placeholder": "", "type": "text", "value": ""}, {"name": "tags", "label": "Tags", "help_string": "A comma separated list indicating what tags, if any, should be used to filter the list of monitors by scope. (optional)", "required": false, "format_type": "text", "default_value": "", "placeholder": "", "type": "text", "value": ""}, {"name": "unaggregated", "label": "Unaggregated", "help_string": "Set unaggregated to true to return all events within the specified [start,end] timeframe. ", "required": false, "format_type": "checkbox", "default_value": true, "type": "checkbox", "value": true}], "data_inputs_options": [{"type": "customized_var", "name": "start_time", "title": "Start Time", "description": "This is the start time from where you want to ingest the data. Please enter UTC time. Example Format: 2020-02-08 00:00:00", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "", "placeholder": "2020-02-08 00:00:00"}, {"type": "customized_var", "name": "end_time", "title": "End Time", "description": "This is the end time to where you want to ingest the data. It could be a future time. Please enter UTC time. Example Format: 2030-03-08 22:11:59", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "", "placeholder": "2030-03-08 22:11:59"}, {"type": "customized_var", "name": "priority", "title": "Priority", "description": "Priority of your events. (optional)", "required_on_edit": false, "required_on_create": false, "possible_values": [{"value": "low", "label": "low"}, {"value": "normal", "label": "normal"}, {"value": "none", "label": "none"}], "format_type": "radiogroup", "default_value": "none"}, {"type": "customized_var", "name": "sources", "title": "Sources", "description": "A comma separated string of sources. (optional)", "required_on_edit": false, "required_on_create": false, "format_type": "text", "default_value": "", "placeholder": ""}, {"type": "customized_var", "name": "tags", "title": "Tags", "description": "A comma separated list indicating what tags, if any, should be used to filter the list of monitors by scope. (optional)", "required_on_edit": false, "required_on_create": false, "format_type": "text", "default_value": "", "placeholder": ""}, {"type": "customized_var", "name": "unaggregated", "title": "Unaggregated", "description": "Set unaggregated to true to return all events within the specified [start,end] timeframe. ", "required_on_edit": false, "required_on_create": false, "format_type": "checkbox", "default_value": true}], "code": "# encoding = utf-8\n\nimport os\nimport sys\nimport time\nimport datetime\nimport requests\nimport json\n\n'''\n    IMPORTANT\n    Edit only the validate_input and collect_events functions.\n    Do not edit any other part in this file.\n    This file is generated only once when creating the modular input.\n'''\n'''\n# For advanced users, if you want to create single instance mod input, uncomment this method.\ndef use_single_instance_mode():\n    return True\n'''\n\n\ndef get_slice_time(start, end, steps):\n    time_list = []\n    chunks = range(start, end, steps)\n    counter = 0\n    for chunk in chunks:\n        counter += 1\n        if len(chunks) is counter:\n            time_list.append((chunk, end))\n        else:\n            time_list.append((chunk, chunk+steps-1))\n    return time_list\n\n\ndef build_event_url(datadog_site, start, end, priority, sources, tags, unaggregated):\n    endpoint = \"https://api.datadoghq.{}/api/v1/events?\".format(datadog_site)\n    param = \"start=\" + str(start)\n    param += \"&end=\" + str(end)\n    if priority:\n        param += \"&priority=\" + str(priority)\n    if sources:\n        param += \"&sources=\" + str(sources)\n    if tags:\n        param += \"&tags=\" + str(tags)\n    if unaggregated:\n        param += \"&unaggregated=true\"\n    url = endpoint + param\n    return url\n\n\ndef validate_input(helper, definition):\n    \"\"\"Implement your own validation logic to validate the input stanza configurations\"\"\"\n    # This example accesses the modular input variable\n    # api_key = definition.parameters.get('api_key', None)\n    # app_key = definition.parameters.get('app_key', None)\n    # start_time = definition.parameters.get('start_time', None)\n    # end_time = definition.parameters.get('end_time', None)\n    # priority = definition.parameters.get('priority', None)\n    # sources = definition.parameters.get('sources', None)\n    # tags = definition.parameters.get('tags', None)\n    # unaggregated = definition.parameters.get('unaggregated', None)\n    start_time = definition.parameters.get('start_time', None)\n    end_time = definition.parameters.get('end_time', None)\n\n    priority = definition.parameters.get('priority', None)\n\n    try:\n        datetime.datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S')\n        datetime.datetime.strptime(end_time, '%Y-%m-%d %H:%M:%S')\n    except ValueError:\n        raise ValueError(\n            \"Incorrect data format, should be YYYY-MM-DD hh:mm:ss\")\n    pass\n\n\ndef collect_events(helper, ew):\n    \"\"\"Implement your data collection logic here\n\n    # The following examples get the arguments of this input.\n    # Note, for single instance mod input, args will be returned as a dict.\n    # For multi instance mod input, args will be returned as a single value.\n    opt_api_key = helper.get_arg('api_key')\n    opt_app_key = helper.get_arg('app_key')\n    opt_start_time = helper.get_arg('start_time')\n    opt_end_time = helper.get_arg('end_time')\n    opt_priority = helper.get_arg('priority')\n    opt_sources = helper.get_arg('sources')\n    opt_tags = helper.get_arg('tags')\n    opt_unaggregated = helper.get_arg('unaggregated')\n    # In single instance mode, to get arguments of a particular input, use\n    opt_api_key = helper.get_arg('api_key', stanza_name)\n    opt_app_key = helper.get_arg('app_key', stanza_name)\n    opt_start_time = helper.get_arg('start_time', stanza_name)\n    opt_end_time = helper.get_arg('end_time', stanza_name)\n    opt_priority = helper.get_arg('priority', stanza_name)\n    opt_sources = helper.get_arg('sources', stanza_name)\n    opt_tags = helper.get_arg('tags', stanza_name)\n    opt_unaggregated = helper.get_arg('unaggregated', stanza_name)\n\n    # get input type\n    helper.get_input_type()\n\n    # The following examples get input stanzas.\n    # get all detailed input stanzas\n    helper.get_input_stanza()\n    # get specific input stanza with stanza name\n    helper.get_input_stanza(stanza_name)\n    # get all stanza names\n    helper.get_input_stanza_names()\n\n    # The following examples get options from setup page configuration.\n    # get the loglevel from the setup page\n    loglevel = helper.get_log_level()\n    # get proxy setting configuration\n    proxy_settings = helper.get_proxy()\n    # get account credentials as dictionary\n    account = helper.get_user_credential_by_username(\"username\")\n    account = helper.get_user_credential_by_id(\"account id\")\n    # get global variable configuration\n    global_userdefined_global_var = helper.get_global_setting(\"userdefined_global_var\")\n\n    # The following examples show usage of logging related helper functions.\n    # write to the log for this modular input using configured global log level or INFO as default\n    helper.log(\"log message\")\n    # write to the log using specified log level\n    helper.log_debug(\"log message\")\n    helper.log_info(\"log message\")\n    helper.log_warning(\"log message\")\n    helper.log_error(\"log message\")\n    helper.log_critical(\"log message\")\n    # set the log level for this modular input\n    # (log_level can be \"debug\", \"info\", \"warning\", \"error\" or \"critical\", case insensitive)\n    helper.set_log_level(log_level)\n\n    # The following examples send rest requests to some endpoint.\n    response = helper.send_http_request(url, method, parameters=None, payload=None,\n                                        headers=None, cookies=None, verify=True, cert=None,\n                                        timeout=None, use_proxy=True)\n    # get the response headers\n    r_headers = response.headers\n    # get the response body as text\n    r_text = response.text\n    # get response body as json. If the body text is not a json string, raise a ValueError\n    r_json = response.json()\n    # get response cookies\n    r_cookies = response.cookies\n    # get redirect history\n    historical_responses = response.history\n    # get response status code\n    r_status = response.status_code\n    # check the response status, if the status is not sucessful, raise requests.HTTPError\n    response.raise_for_status()\n\n    # The following examples show usage of check pointing related helper functions.\n    # save checkpoint\n    helper.save_check_point(key, state)\n    # delete checkpoint\n    helper.delete_check_point(key)\n    # get checkpoint\n    state = helper.get_check_point(key)\n\n    # To create a splunk event\n    helper.new_event(data, time=None, host=None, index=None, source=None, sourcetype=None, done=True, unbroken=True)\n    \"\"\"\n\n    '''\n    # The following example writes a random number as an event. (Multi Instance Mode)\n    # Use this code template by default.\n    import random\n    data = str(random.randint(0,100))\n    event = helper.new_event(source=helper.get_input_type(), index=helper.get_output_index(), sourcetype=helper.get_sourcetype(), data=data)\n    ew.write_event(event)\n    '''\n\n    '''\n    # The following example writes a random number as an event for each input config. (Single Instance Mode)\n    # For advanced users, if you want to create single instance mod input, please use this code template.\n    # Also, you need to uncomment use_single_instance_mode() above.\n    import random\n    input_type = helper.get_input_type()\n    for stanza_name in helper.get_input_stanza_names():\n        data = str(random.randint(0,100))\n        event = helper.new_event(source=input_type, index=helper.get_output_index(stanza_name), sourcetype=helper.get_sourcetype(stanza_name), data=data)\n        ew.write_event(event)\n    '''\n\n    opt_api_key = helper.get_global_setting('api_key')\n    opt_app_key = helper.get_global_setting('app_key')\n    opt_datadog_site = helper.get_global_setting('datadog_site')\n    \n    opt_start_time = helper.get_arg('start_time')\n    opt_end_time = helper.get_arg('end_time')\n\n    opt_interval = helper.get_arg('interval')\n    \n    helper.log_debug(\"[-] DataDog Events: datadog site {}\".format(opt_datadog_site))\n    \n    helper.log_debug(\"[-] DataDog Events: UI start time {}\".format(opt_start_time))\n    helper.log_debug(\"[-] DataDog Events: UI end time {}\".format(opt_end_time))\n\n    opt_start_time = datetime.datetime.strptime(\n        opt_start_time, '%Y-%m-%d %H:%M:%S')\n    opt_start_time = int(\n        (opt_start_time - datetime.datetime(1970, 1, 1)).total_seconds())\n\n    opt_end_time = datetime.datetime.strptime(\n        opt_end_time, '%Y-%m-%d %H:%M:%S')\n    opt_end_time = int(\n        (opt_end_time - datetime.datetime(1970, 1, 1)).total_seconds())\n\n    opt_priority = helper.get_arg('priority')\n    opt_sources = helper.get_arg('sources')\n    opt_tags = helper.get_arg('tags')\n    opt_unaggregated = helper.get_arg('unaggregated')\n\n    # Slice\n    steps = 60*60*24\n\n    #payload = {}\n    headers = {\n        'Content-type': 'application/json',\n        'DD-API-KEY': opt_api_key,\n        'DD-APPLICATION-KEY': opt_app_key\n    }\n\n    # checkpoint key\n    current_url = build_event_url(opt_datadog_site, opt_start_time, opt_end_time,\n                            opt_priority, opt_sources, opt_tags, opt_unaggregated)\n    key = \"{}_DATADOG_EVENTS_processing\".format(\n        helper.get_input_stanza_names())\n    last_ran_key = \"last_ran_{}\".format(key)\n\n    # check checkpoint\n\n    helper.log_debug(\"[-] DataDog Events: check checkpoint\")\n    helper.log_debug(\n        \"[-] DataDog Events: Last run time: {}\".format(helper.get_check_point(last_ran_key)))\n\n    now = datetime.datetime.utcnow()\n    now = int((now - datetime.datetime(1970, 1, 1)).total_seconds())\n    helper.log_debug(\"[-] DataDog Events: now - {}\".format(now))\n\n    if helper.get_check_point(last_ran_key) is None:\n        helper.save_check_point(last_ran_key, opt_start_time)\n    else:\n        opt_start_time = int(helper.get_check_point(last_ran_key) + 1)\n\n    if now < opt_end_time:\n        opt_end_time = now\n\n    helper.log_debug(\"[-] DataDog Events: opt_start_time - {}\".format(opt_start_time))\n    helper.log_debug(\"[-] DataDog Events: opt_end_time - {}\".format(opt_end_time))\n\n    helper.log_debug(\n        \"\\t[-] DataDog Events: last run checkpoint: {} -- value: {}\".format(last_ran_key, helper.get_check_point(last_ran_key)))\n\n    # Pageing\n\n    time_list = get_slice_time(int(opt_start_time), int(opt_end_time), steps)\n\n    helper.log_debug(\n        \"Processing DataDog Events time_list: {}\".format(len(time_list)))\n    for time in time_list:\n        helper.log_debug(\n            \"\\t[-] DataDog Events:In for: processing time {}\".format(time))\n        # build url according to user's inputs\n        url = build_event_url(opt_datadog_site, time[0], time[1], opt_priority,\n                        opt_sources, opt_tags, opt_unaggregated)\n\n        response = helper.send_http_request(url, \"GET\", parameters=None, payload=None,\n                                            headers=headers, cookies=None, verify=True, cert=None, timeout=None, use_proxy=True)\n\n        if response.status_code != 200:\n            helper.log_debug(\n                \"\\t[-] DataDog Events API Error: {}\".format(response.text))\n\n        events = response.json()\n        helper.log_debug(\"\\t[-] DataDog Events Raw Events: {}\".format(events))\n\n        if \"events\" in events:\n            for event in events['events']:\n\n                helper.log_debug(\n                    \"\\t\\t[-] DataDog Events Each Individual Event: {}\".format(event))\n                try:\n                    event['ddhost'] = event['host']\n                    event['ddsource'] = event['source']\n                    del event['host']\n                    del event['source']\n                except Exception as e:\n                    helper.log_debug(\n                        \"\\t[-] Try Block 1: DataDog Events Exception {}\".format(e))\n                    pass\n\n                try:\n                    event_time = event['date_happened']\n                    event = helper.new_event(json.dumps(\n                        event), time=event_time, host=None, index=None, source=None, sourcetype=\"DataDog Events:events\", done=True, unbroken=True)\n                    ew.write_event(event)\n\n                    # save checkpoint for every event\n                    timestamp = helper.get_check_point(last_ran_key)\n                    if timestamp is None:\n                        timestamp = event_time\n                    else:\n                        timestamp = max(int(timestamp), int(event_time))\n                    helper.save_check_point(last_ran_key, timestamp)\n                    helper.log_debug(\n                        \"[-] DataDog Events: timestamp: {}\".format(timestamp))\n                    helper.log_debug(\n                        \"[-] DataDog Events: Last run time saved: {}\".format(helper.get_check_point(last_ran_key)))\n                except Exception as e:\n                    helper.log_debug(\n                        \"\\t[-] Try Block 2: DataDog Events Exception {}\".format(e))\n        else:\n            helper.log_debug(\"\\t[-] No events to retrieve for {}.\".format(url))\n", "customized_options": [{"name": "start_time", "value": "2020-05-10 08:00:00"}, {"name": "end_time", "value": "2020-05-21 8:00:00"}, {"name": "priority", "value": "none"}, {"name": "sources", "value": ""}, {"name": "tags", "value": ""}, {"name": "unaggregated", "value": true}], "uuid": "81420348e8a544fcab43c4d9f61eb8ab", "sample_count": 0}, {"index": "default", "sourcetype": "datadog:metric:inventory", "interval": "60", "use_external_validation": true, "streaming_mode_xml": true, "name": "datadog_metric_inventory", "title": "datadog metric inventory", "description": "Input for Datadog Metrics endpoints", "type": "customized", "parameters": [{"name": "query", "label": "Query", "help_string": "Query string", "required": true, "possible_values": [{"value": "system.core.system{*}by{host}", "label": "system.core.system{*}by{host}"}, {"value": "apache.net.hits{*}by{host}", "label": "apache.net.hits{*}by{host}"}], "format_type": "dropdownlist", "default_value": "", "placeholder": "", "type": "dropdownlist", "value": "system.core.system{*}by{host}"}, {"name": "custom_metrics", "label": "Custom Metrics (optional)", "help_string": "Note: You may use \"Custom Metrics\" parameters from datadog (https://docs.datadoghq.com/integrations/system/) to override pre-populated \"Query\" parameter.", "required": false, "format_type": "text", "default_value": "", "placeholder": "i.e. aws.ec2.cpuutilization{*}by{host}", "type": "text", "value": "system.cpu.idle{*}by{host}"}, {"name": "start_time", "label": "Start Time", "help_string": "This is the start time from where you want to ingest the data. Please enter UTC time. Example Format: 2020-02-08 00:00:00", "required": true, "format_type": "text", "default_value": "", "placeholder": "2020-02-08 00:00:00", "type": "text", "value": "2020-05-10 08:00:00"}, {"required": true, "name": "duration", "label": "Duration (To)", "default_value": "", "placeholder": "", "help_string": "", "type": "text", "format_type": "text", "value": "1"}, {"required": false, "name": "duration_unit", "label": "Duration (To) Unit", "default_value": "Second", "placeholder": "", "help_string": "", "possible_values": [{"value": "Second", "label": "Second"}, {"value": "Minute", "label": "Minute"}, {"value": "Hour", "label": "Hour"}, {"label": "Day", "value": "Day"}, {"label": "Week", "value": "Week"}, {"label": "Year", "value": "Year"}], "type": "dropdownlist", "format_type": "dropdownlist", "value": "Hour"}], "data_inputs_options": [{"type": "customized_var", "name": "query", "title": "Query", "description": "Query string", "required_on_edit": false, "required_on_create": true, "possible_values": [{"value": "system.core.system{*}by{host}", "label": "system.core.system{*}by{host}"}, {"value": "apache.net.hits{*}by{host}", "label": "apache.net.hits{*}by{host}"}], "format_type": "dropdownlist", "default_value": "", "placeholder": ""}, {"type": "customized_var", "name": "custom_metrics", "title": "Custom Metrics (optional)", "description": "Note: You may use \"Custom Metrics\" parameters from datadog (https://docs.datadoghq.com/integrations/system/) to override pre-populated \"Query\" parameter.", "required_on_edit": false, "required_on_create": false, "format_type": "text", "default_value": "", "placeholder": "i.e. aws.ec2.cpuutilization{*}by{host}"}, {"type": "customized_var", "name": "start_time", "title": "Start Time", "description": "This is the start time from where you want to ingest the data. Please enter UTC time. Example Format: 2020-02-08 00:00:00", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "", "placeholder": "2020-02-08 00:00:00"}, {"type": "customized_var", "name": "duration", "title": "Duration (To)", "description": "", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "", "placeholder": ""}, {"type": "customized_var", "name": "duration_unit", "title": "Duration (To) Unit", "description": "", "required_on_edit": false, "required_on_create": false, "possible_values": [{"value": "Second", "label": "Second"}, {"value": "Minute", "label": "Minute"}, {"value": "Hour", "label": "Hour"}, {"label": "Day", "value": "Day"}, {"label": "Week", "value": "Week"}, {"label": "Year", "value": "Year"}], "format_type": "dropdownlist", "default_value": "Second", "placeholder": ""}], "code": "\n# encoding = utf-8\nimport os\nimport sys\nimport time\nimport datetime\nimport requests\nimport re\nimport json\nfrom datetime import datetime, timedelta \n\n'''\n    IMPORTANT\n    Edit only the validate_input and collect_events functions.\n    Do not edit any other part in this file.\n    This file is generated only once when creating the modular input.\n'''\n'''\n# For advanced users, if you want to create single instance mod input, uncomment this method.\ndef use_single_instance_mode():\n    return True\n'''\n\ndef get_slice_time(start, end, steps):\n    time_list = []\n    chunks = range(start, end, steps)\n    counter = 0\n    for chunk in chunks:\n        counter += 1\n        if len(chunks) is counter:\n            time_list.append((chunk, end))\n        else:\n            time_list.append((chunk, chunk+steps-1))\n    return time_list\n\n\ndef build_metric_url(datadog_site, start, end, query):\n    endpoint = \"https://app.datadoghq.{}/api/v1/query?\".format(datadog_site)\n    param = \"&from=\" + str(start)\n    param += \"&to=\" + str(end)\n    param += \"&query=\" + query\n    url = endpoint + param\n    return url\n\ndef validate_input(helper, definition):\n    \"\"\"Implement your own validation logic to validate the input stanza configurations\"\"\"\n    # This example accesses the modular input variable\n    # start_time = definition.parameters.get('start_time', None)\n    # end_time = definition.parameters.get('end_time', None)\n    # query = definition.parameters.get('query', None)\n    # custom_metrics = definition.parameters.get('custom_metrics', None)\n    pass\n\ndef collect_events(helper, ew):\n    # Get User Input\n    opt_api_key = helper.get_global_setting('api_key')\n    opt_app_key = helper.get_global_setting('app_key')\n    opt_datadog_site = helper.get_global_setting('datadog_site')\n    \n    opt_start_time = helper.get_arg('start_time')\n    # opt_end_time = helper.get_arg('end_time')\n    opt_duration = helper.get_arg('duration')\n    opt_duration_unit = helper.get_arg('duration_unit')\n    \n    opt_custom_metrics  = helper.get_arg('custom_metrics')\n    if opt_custom_metrics:\n        query = opt_custom_metrics\n    else:\n        query = helper.get_arg('query')\n    \n    # Generate sourcetype based on datadog metrics\n    # i.e. datadog:metric:cpu_system, datadog:metric:inventory\n    runtime_sourcetype = \"datadog:metric:\" +  query.split('{')[0].replace('.','_');\n    \n    helper.log_debug(\"[-] DataDog Metrics: datadog site {}\".format(opt_datadog_site))\n    \n    # Parse and construct humar readable \"to\" parameter\n    to_with_unit = int(re.search(r'\\d+', opt_duration).group())\n    if \"Sec\" in opt_duration_unit:\n        parsed = \"second\"\n        duration = to_with_unit\n    elif \"Min\" in opt_duration_unit:\n        parsed = \"minute\"\n        duration = to_with_unit * 60\n    elif \"Hour\" in opt_duration_unit:\n        parsed = \"hour\"\n        duration = to_with_unit * 60 * 60\n    elif \"Day\" in opt_duration_unit:\n        parsed = \"day\"\n        duration = to_with_unit * 60 * 60 * 24\n    elif \"Week\" in opt_duration_unit:\n        parsed = \"week\"\n        duration = to_with_unit * 60 * 60 * 24 * 7\n    elif \"Year\" in opt_duration_unit:\n        parsed = \"year\"\n        duration = to_with_unit * 60 * 60 * 24 * 365\n    else:\n        parsed = \"default\"\n        duration = to_with_unit\n    \n    helper.log_debug(\"[-] DataDog Metrics: duration {}\".format(duration))\n\n\n\n    # TODO add checkpoint\n    # set checkpoint key\n    key = \"{}_DATADOG_METRICS_processing_for_{}_{}_{}_{}\".format(\n        helper.get_input_stanza_names(), opt_start_time, opt_duration, opt_duration_unit, query)\n\n    # check checkpoint\n\n    helper.log_debug(\"[-] DataDog Metrics: check checkpoint\")\n\n    helper.log_debug(\n        \"[-] DataDog Metrics: Last start time: {}\".format(helper.get_check_point(key)))\n    \n    # Fist time: save end time in checkpoint\n    if helper.get_check_point(key) is None:\n        # convert start time to datetime type\n        opt_start_time = datetime.strptime(\n            opt_start_time, '%Y-%m-%d %H:%M:%S')\n        # calculate end time by adding duration to start time\n        opt_end_time = expired_time = (\n            opt_start_time + timedelta(seconds=duration))\n\n        # convert start time and end time to integer\n        opt_start_time = int(\n            (opt_start_time - datetime(1970, 1, 1)).total_seconds())\n        opt_end_time = int(\n            (opt_end_time - datetime(1970, 1, 1)).total_seconds())\n        helper.save_check_point(key, opt_end_time)\n    else:\n        # shift start time by 1 second\n        opt_start_time = int(helper.get_check_point(key) + 1)\n        opt_end_time = opt_start_time + duration\n       \n    helper.log_debug(\"[-] DataDog Metrics: start time {}\".format(opt_start_time))\n    helper.log_debug(\"[-] DataDog Metrics: end time {}\".format(opt_end_time))\n        \n    # build url according to user's inputs\n    url = build_metric_url(opt_datadog_site, opt_start_time, opt_end_time, query)\n    \n    headers = {\n        'Content-type': 'application/json',\n        'DD-API-KEY': opt_api_key,\n        'DD-APPLICATION-KEY': opt_app_key\n    }\n\n    response = helper.send_http_request(url, \"GET\", parameters=None, payload=None,\n                                        headers=headers, cookies=None, verify=True, cert=None, timeout=None, use_proxy=True)\n\n    if response.status_code != 200:\n        helper.log_error(\n            \"\\t[-] DataDog Metrics API Error: {}\".format(response.text))\n        e = helper.new_event(source=helper.get_input_type(), index=helper.get_output_index(), sourcetype=\"datadog:metric:errors\", data=json.dumps(json.loads(resp.text)))\n        try:\n            ew.write_event(e)\n        except Exception as ex:\n            raise ex\n    else:\n        events = response.json()\n        if events['status'] == \"ok\":\n            if len(events['series']) > 0:\n                for event in events['series']:\n                    # event['Timestamp'] = str(event['start'])[:-3]\n                    event['Timestamp'] = str(event['start']/1000)\n            \n                    # Calculated field: pointlist_average = sum of pointlist entries / count of pointlist entries\n                    total=0\n                    for point in event['pointlist']:\n                        total += point[1]\n                   \n                    helper.log_info(\"type of totla -- {}\".format(type(total)))\n                    helper.log_info(\"totla -- {}\".format(total))\n            \n                    event['pointlist_count']    = len(event['pointlist'])\n                    event['pointlist_average']  = total/event['pointlist_count']\n                    \n                    helper.log_info(\"event['pointlist_count'] -- {}\".format(len(event['pointlist'])))\n                    helper.log_info(\"event['length'] -- {}\".format(event['length']))\n                    helper.log_info(\"event['pointlist_average'] -- {}\".format(event['pointlist_average']))\n            \n                    # Calcuated field: Host\n                    runtime_host = re.split(':', event['scope'])[1]\n                    helper.log_debug(\"event --- {}\".format(event))\n            \n                    # Build Event\n                    # e = helper.new_event(source=helper.get_input_type(), index=helper.get_output_index(), sourcetype=helper.get_sourcetype(), data=json.dumps(event))\n                    e = helper.new_event(host=runtime_host, source=helper.get_input_type(), index=helper.get_output_index(), sourcetype=runtime_sourcetype, data=json.dumps(event))\n                    try:\n                        ew.write_event(e)\n                    except Exception as ex:\n                        raise ex\n            else:\n                helper.log_debug(\"events --- {}\".format(events))\n                events['Timestamp'] = str(events['from_date']/1000)\n            \n                    # Calculated field: pointlist_average = sum of pointlist entries / count of pointlist entries\n                events['pointlist_count']    = 0\n                events['pointlist_average']  = 0\n        \n                # Build Event\n                # e = helper.new_event(source=helper.get_input_type(), index=helper.get_output_index(), sourcetype=helper.get_sourcetype(), data=json.dumps(event))\n                e = helper.new_event(host=None, source=helper.get_input_type(), index=helper.get_output_index(), sourcetype=runtime_sourcetype, data=json.dumps(events))\n                try:\n                    ew.write_event(e)\n                except Exception as ex:\n                    raise ex\n        else:\n            helper.log_error(\n            \"\\t[-] DataDog Metrics API Error: {}\".format(events[\"error\"]))\n            e = helper.new_event(source=helper.get_input_type(), index=helper.get_output_index(), sourcetype=\"datadog:metric:errors\", data=json.dumps(events))\n            try:\n                ew.write_event(e)\n            except Exception as ex:\n                raise ex\n            \n            \n", "customized_options": [{"name": "query", "value": "system.core.system{*}by{host}"}, {"name": "custom_metrics", "value": "system.cpu.idle{*}by{host}"}, {"name": "start_time", "value": "2020-05-10 08:00:00"}, {"name": "duration", "value": "1"}, {"name": "duration_unit", "value": "Hour"}], "uuid": "4c5fd4ff70d642f08c80b8056259d38e", "sample_count": 0}]}, "global_settings_builder": {"global_settings": {"proxy_settings": {"proxy_type": "http"}, "log_settings": {"log_level": "DEBUG"}, "customized_settings": [{"required": true, "name": "api_key", "label": "API Key", "placeholder": "", "default_value": "", "help_string": "Datadog API Key", "type": "password", "format_type": "password", "value": ""}, {"required": true, "name": "app_key", "label": "APP Key", "placeholder": "", "default_value": "", "help_string": "Datadog Application Key", "type": "password", "format_type": "password", "value": ""}, {"required": true, "name": "datadog_site", "label": "Datadog Site", "default_value": "", "placeholder": "", "help_string": "Please enter \"com\" if you are on Datadog US site or enter \"eu\" if you are on Datadog EU site.", "type": "text", "format_type": "text", "value": "com"}]}}, "sourcetype_builder": {"datadog:event:stream": {"metadata": {"event_count": 0, "data_input_name": "datadog_event_stream", "extractions_count": 0, "cims_count": 0}}, "datadog:metric:inventory": {"metadata": {"event_count": 0, "data_input_name": "datadog_metric_inventory", "extractions_count": 0, "cims_count": 0}}}, "validation": {"validators": ["best_practice_validation", "data_model_mapping_validation", "app_cert_validation"], "status": "job_finished", "validation_id": "v_1598630463_25", "progress": 1.0}}